{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this kernel I’ll test fake & real news dataset – I’ll use different classification methods ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import nltk\n",
    "import nltk as nlp\n",
    "import string\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine fake/real to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "real = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\n",
    "fake = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\n",
    "fake['target'] = 'fake'\n",
    "real['target'] = 'real'\n",
    "txt_news = pd.concat([fake, real]).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date target  \n",
       "0  December 31, 2017   fake  \n",
       "1  December 31, 2017   fake  \n",
       "2  December 30, 2017   fake  \n",
       "3  December 29, 2017   fake  \n",
       "4  December 25, 2017   fake  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_news.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check if we have null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      False\n",
       "text       False\n",
       "subject    False\n",
       "date       False\n",
       "target     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_news.isnull().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples : 44898\n"
     ]
    }
   ],
   "source": [
    "print('Number of examples : ' + str(txt_news.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=txt_news.target\n",
    "#txt_news=txt_news.drop(columns=['date', 'subject','target'])\n",
    "txt_news['total']=txt_news['text']+' '+txt_news['subject']+txt_news['title']\n",
    "\n",
    "target_binary=[]\n",
    "for false_true in target :\n",
    "    if false_true==\"fake\":\n",
    "        target_binary.append(0)\n",
    "    else :\n",
    "        target_binary.append(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf\n",
    "#transformer = TfidfTransformer(smooth_idf=False)\n",
    "#count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "#counts = count_vectorizer.fit_transform(txt_news['total'].values)\n",
    "#tfidf = transformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(txt_news['total'], target, test_size=0.25, random_state=42) #txt_news['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy KNeighborsClassifier: 89.942%\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('CountV', CountVectorizer()),\n",
    "                 ('TfidfT', TfidfTransformer()),\n",
    "                 ('clf', KNeighborsClassifier(n_neighbors = 30,weights = 'distance'))])#algorithm = 'brute'\n",
    "\n",
    "model = pipeline.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"accuracy KNeighborsClassifier: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy SVM: 99.581%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline = Pipeline([('CountV', CountVectorizer()),\n",
    "                 ('TfidfT', TfidfTransformer()),\n",
    "                 ('clf', LinearSVC(C=12, random_state=7))])#loss=\"hinge\"\n",
    "\n",
    "model = pipeline.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"accuracy SVM: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADABoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy AdaBoostClassifier: 99.661%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "pipeline = Pipeline([('CountV', CountVectorizer()),\n",
    "                 ('TfidfT', TfidfTransformer()),\n",
    "                 ('clf', AdaBoostClassifier())])\n",
    "\n",
    "model = pipeline.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"accuracy AdaBoostClassifier: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy MultinomialNB: 94.396%\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('CountV', CountVectorizer()),\n",
    "                 ('TfidfT', TfidfTransformer()),\n",
    "                 ('clf', MultinomialNB(alpha=0.5))])\n",
    "\n",
    "model = pipeline.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"accuracy MultinomialNB: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy XGBClassifier: 99.528%\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('CountV', CountVectorizer()),\n",
    "                 ('TfidfT', TfidfTransformer()),\n",
    "                 ('clf', XGBClassifier(\n",
    "                                                   learning_rate = 0.015,\n",
    "                                                   n_estimators = 18,\n",
    "                                                   max_depth = 7,\n",
    "                                                   random_state=42))])#loss = 'deviance',\n",
    "\n",
    "model = pipeline.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"accuracy XGBClassifier: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy LGBMClassifier: 99.546%\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = LGBMClassifier(objective='multiclass', random_state=5)\n",
    "\n",
    "pipe = Pipeline([('CountV', CountVectorizer()),\n",
    "                 ('TfidfT', TfidfTransformer()),\n",
    "                 ('clf', LGBMClassifier(\n",
    "                                                   learning_rate = 0.01,\n",
    "                                                   n_estimators = 18,\n",
    "                                                   max_depth = 7,\n",
    "                                                   random_state=42))])#loss = 'deviance',\n",
    "\n",
    "model = pipe.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"accuracy LGBMClassifier: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM with Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['CountV', 'TfidfT', 'clf']\n",
      "parameters:\n",
      "{'clf__learning_rate': [0.01],\n",
      " 'clf__max_depth': [7, 9],\n",
      " 'clf__n_estimators': [15, 20],\n",
      " 'clf__random_state': [42]}\n",
      "done in 713.100s\n",
      "\n",
      "Best score: 0.995\n",
      "Best parameters set:\n",
      "\tclf__learning_rate: 0.01\n",
      "\tclf__max_depth: 9\n",
      "\tclf__n_estimators: 20\n",
      "\tclf__random_state: 42\n",
      "accuracy LGBMClassifier with GridSearchCV: 99.519%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "pipeline = Pipeline([('CountV', CountVectorizer()),\n",
    "                 ('TfidfT', TfidfTransformer()),\n",
    "                 ('clf', XGBClassifier()),])\n",
    "\n",
    "parameters = {}\n",
    "parameters['clf__learning_rate'] = [0.01]\n",
    "parameters['clf__n_estimators'] = [15,20]\n",
    "parameters['clf__max_depth'] = [7,9]\n",
    "parameters['clf__random_state'] = [42]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "CV = GridSearchCV(pipe, parameters, n_jobs= 1) # scoring = 'mean_absolute_error'\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "\n",
    "model = CV.fit(x_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % CV.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = CV.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "\n",
    "prediction = CV.predict(x_test)\n",
    "print(\"accuracy LGBMClassifier with GridSearchCV: {}%\".format(round(accuracy_score(y_test, prediction)*100,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy RandomForestClassifier: 99.546%\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('CountV', CountVectorizer()),\n",
    "                 ('TfidfT', TfidfTransformer()),\n",
    "                 ('clf', RandomForestClassifier(criterion= \"entropy\"))])\n",
    "\n",
    "model = pipe.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(\"accuracy RandomForestClassifier: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = txt_news.total\n",
    "Y = txt_news.target\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get longest txt length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51904\n"
     ]
    }
   ],
   "source": [
    "len_size=0\n",
    "for f in X:\n",
    "    if len_size<len(f) :\n",
    "        len_size=len(f)\n",
    "        \n",
    "print(str(len_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total      51904\n",
      "text       51794\n",
      "title        286\n",
      "date         149\n",
      "subject       15\n",
      "target         4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series({c: txt_news[c].map(lambda x: len(str(x))).max() for c in txt_news}).sort_values(ascending =False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\n",
    "max_words = 500\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "def LSTM_MODEL():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_lstm = LSTM_MODEL()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model \n",
    "plot_model(model_lstm, to_file='lstm_png1.png')\n",
    "model_lstm.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30530 samples, validate on 7633 samples\n",
      "Epoch 1/10\n",
      "30530/30530 [==============================] - 22s 713us/step - loss: 0.1822 - accuracy: 0.9312 - val_loss: 0.0853 - val_accuracy: 0.9699\n",
      "Epoch 2/10\n",
      "30530/30530 [==============================] - 20s 642us/step - loss: 0.0654 - accuracy: 0.9774 - val_loss: 0.0501 - val_accuracy: 0.9823\n",
      "Epoch 3/10\n",
      "30530/30530 [==============================] - 19s 612us/step - loss: 0.0504 - accuracy: 0.9830 - val_loss: 0.1144 - val_accuracy: 0.9524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f7cf2cb8a50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(sequences_matrix,Y_train,batch_size=256,epochs=10,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6735/6735 [==============================] - 5s 768us/step\n",
      "Accuracy LSTM: 0.95\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "accr = model_lstm.evaluate(test_sequences_matrix,Y_test)\n",
    "print('Accuracy LSTM: {:0.2f}'.format(accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "def GRU_model():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = GRU(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "\n",
    "model_gru = GRU_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_gru, to_file='gru_png1.png')\n",
    "model_gru.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30530 samples, validate on 7633 samples\n",
      "Epoch 1/10\n",
      "30530/30530 [==============================] - 25s 803us/step - loss: 0.1945 - accuracy: 0.9275 - val_loss: 0.0732 - val_accuracy: 0.9739\n",
      "Epoch 2/10\n",
      "30530/30530 [==============================] - 23s 743us/step - loss: 0.0670 - accuracy: 0.9775 - val_loss: 0.0610 - val_accuracy: 0.9765\n",
      "Epoch 3/10\n",
      "30530/30530 [==============================] - 24s 773us/step - loss: 0.0546 - accuracy: 0.9815 - val_loss: 0.1233 - val_accuracy: 0.9662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f7cec4eedd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru.fit(sequences_matrix,Y_train,batch_size=256,epochs=10,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6735/6735 [==============================] - 7s 1ms/step\n",
      "Accuracy GRU: 0.96\n"
     ]
    }
   ],
   "source": [
    "accr = model_gru.evaluate(test_sequences_matrix,Y_test)\n",
    "print('Accuracy GRU: {:0.2f}'.format(accr[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
